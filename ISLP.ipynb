{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c148d14",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10614c55",
   "metadata": {},
   "source": [
    "Supervised statistical learning involves building statistical models to predict or estimate an output based on some inputs.\n",
    "\n",
    "We can characterize variables as either quantitative or qualitative; quantitative variables take on numerical values (e.g. age, income, height, density population) while qualitative variables take on categorical values (e.g. marital status, type of product bought). We usually refer to problems with a quantitative response as regression problems, and to problems with a qualitative response as classification problems. For example, we use least squares linear regression with quantitative response and logistic regression with qualitative responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f61ac8",
   "metadata": {},
   "source": [
    "# 2. Model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4c4e85",
   "metadata": {},
   "source": [
    "There is no particular statistical method that dominates all the others over all possible datasets. It is really important to decide for a particular dataset which method is the best (which method produces the best results). In many situations, a set of inputs $X = (X_1,X_2, \\ldots, X_p)$ are readily available, but the output $Y$ cannot be easily obtained. In this setting, we assume that there is some relationship between $Y$ and $X$, which we can write in the more general form\n",
    "\n",
    "$$ Y = f(X) + \\epsilon $$\n",
    "\n",
    "where $f$ is some fixed but unknown function of $X_1, X_2, \\ldots, X_p$ and $\\epsilon$ is a random error term, which we assume to be independent of $X$ and has mean zero. In this formulation, $f$ represents the _systematic_ information that $X$ provides about $Y$. Since the error term averages to zero (assumption), we can predict $Y$ using \n",
    "\n",
    "$$ \\hat{Y} = \\hat{f}(X) $$\n",
    "\n",
    "where $\\hat{Y} = \\text{E}[Y]$. \n",
    "\n",
    "The accuracy of $\\hat{Y}$ as a prediction for $Y$ depends on two quantities, which we name the _reducible error_ and the _irreducible error_. In general $\\hat{f}$ will not be a perfect estimate for $f$, and this inaccuracy will introduce some error. In this sense, this error is _reducible_ because we can potentially improve the accuracy of $\\hat{f}$ by using the most appropriate statistical learning technique. Even if it were possible to obtain a perfect estimate for $f$, so that our estimated response took the form $\\hat{Y} = f(X)$, our prediction would still have some error in it, the reason is because $Y$ is also a function of $\\epsilon$, which, by definition, cannot be predicted using $X$. Therefore, variability associated with $\\epsilon$ also affects the accuracy of our predictions. This is known as the _irreducible error_, since no matter how well we estimate $f$, we cannot reduce the error introduced by $\\epsilon$.\n",
    "\n",
    "Consider a given estimate $\\hat{f}$ and a set of predictors $X$, which yields the prediction $\\hat{Y} = \\hat{f}(X)$. Assume for a moment that both $\\hat{f}$ and $X$ are fixed, so that the only variability comes from $\\epsilon$. Then, we can see that\n",
    "\n",
    "$$\n",
    "\\begin{align*} \n",
    "    \\text{E}(Y-\\hat{Y}) &= \\text{E}[f(X)+\\epsilon -\\hat{f}(X)]^2 \\\\\n",
    "    &= \\text{E}[(f(X)-\\hat{f}(X))^2+2(f(X)-\\hat{f}(X))\\epsilon +\\epsilon^2] \\\\\n",
    "     &= [f(X)-\\hat{f}(X)]^2 + \\text{Var}(\\epsilon)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\text{E}(Y-\\hat{Y})$ represents the average, or expected value, of the square difference between the predicted and actual value of $Y$ and $\\text{Var}(\\epsilon)$ represents the variance associated with the error term $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a40045",
   "metadata": {},
   "source": [
    "## 2.1. Measuring the quality of fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca917cf0",
   "metadata": {},
   "source": [
    "It is important to have a metric to evaluate the performance of the method in question on a given dataset, i.e., we need to find a way to quantify how close or accurate the predicted response is to the true response. \n",
    "\n",
    "In the regression environment, we tend to use the mean squared error (MSE)\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n}{(y_i-\\hat{f}(x_i))^2}\n",
    "\\end{equation*}\n",
    "    \n",
    "where $\\hat{f}(x_i)$ is the prediction related to the ith observation. It is important to distinguish between two types of MSE: training MSE and test MSE (we don't really care too much if the training MSE is very accurate, we mostly care about the test MSE). We want to choose the method that gives the lowest test MSE as compared to the one who yields the lowest training MSE. \n",
    "\n",
    "As model flexibility increases, the training MSE decreases, but the test MSE may not. When a training method yields a small training MSE but a large test MSE we say we are overfitting the data. This happens because our statistical method is trying too hard to find patterns in the training data that may be caused by the irreducible error (random chance) rather than by true properties of the true unknown function. Hence when we overfit the training data, the test MSE will be large because the method will be trying to model for patterns that don't exist anymore in new data. \n",
    "\n",
    "Sometimes (majority) it is difficult to estimate the test MSE because there's simply no test data available to validate the model with, in this case we can apply a method called cross validation in which we try to estimate the test MSE using the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b3527",
   "metadata": {},
   "source": [
    "We can show that the expected test MSE, for a given value $x_0$, can always be decomposed into the sum of three fundamental quantities: the variance of $\\hat{f}(x_0)$, the square bias of $\\hat{f}(x_0)$, and the variance of the error terms $\\epsilon$. That is, \n",
    "\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\operatorname{E} \\left[ Y-\\hat{Y} \\middle\\vert X = x_0 \\right]^2 &= \\operatorname{E} \\left[f(X)+\\epsilon-\\hat{f}(X) \\middle\\vert X = x_0 \\right]^2 \\\\\n",
    "    &= \\text{E}\\left[f(x_0)+\\epsilon -\\hat{f}(x_0)\\right]^2 \\\\\n",
    "    &= \\text{E}\\left[\\left(f(x_0)-\\hat{f}(x_0)\\right)^2+2\\left(f(x_0)-\\hat{f}(x_0)\\right)\\epsilon +\\epsilon^2\\right] \\\\\n",
    "    &= \\text{E}\\left[\\left(f(x_0)-\\hat{f}(x_0)\\right)^2\\right]+ \\text{E}\\left[2\\left(f(x_0)-\\hat{f}(x_0)\\right)\\epsilon\\right] + \\text{E}\\left[\\epsilon^2\\right] \\\\\n",
    "    &= \\text{E}\\left[\\left(f(x_0)-\\hat{f}(x_0)\\right)^2\\right]+\\text{Var}\\left(\\epsilon\\right) \\quad \\text{(since $\\hat{f}$ is independent of $\\epsilon$)}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "But we can see that \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{E}\\left[\\left(f(x_0)-\\hat{f}(x_0)\\right)^2\\right] &= \\text{E}\\left[{f(x_0)}^2-2f(x_0)\\hat{f}(x_0)+{\\hat{f}(x_0)}^2\\right] \\\\\n",
    "    &= {f(x_0)}^2-2f(x_0)\\text{E}\\left[\\hat{f}(x_0)\\right]+\\text{E}\\left[{\\hat{f}(x_0)}^2\\right] \\\\\n",
    "    &= {f(x_0)}^2-2f(x_0)\\text{E}\\left[\\hat{f}(x_0)\\right]+\\text{E}\\left[{\\hat{f}(x_0)}\\right]^2+\\text{E}\\left[{\\hat{f}(x_0)}^2\\right] - \\text{E}\\left[{\\hat{f}(x_0)}\\right]^2\\\\\n",
    "    &= \\left(f(x_0)-\\text{E}\\left[\\hat{f}(x_0)\\right]\\right)^2 + \\text{Var}\\left(\\hat{f}(x_0)\\right)\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence, merging it all we have that \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\operatorname{E} \\left[ Y-\\hat{Y} \\middle\\vert X = x_0 \\right]^2 &= \\left(f(x_0)-\\text{E}\\left[\\hat{f}(x_0)\\right]\\right)^2 + \\text{Var}\\left(\\hat{f}(x_0)\\right)+\\text{Var}\\left(\\epsilon\\right)\\\\\n",
    "&= \\left(\\text{E}\\left[\\hat{f}(x_0)\\right]-f(x_0)\\right)^2 + \\text{Var}\\left(\\hat{f}(x_0)\\right)+\\text{Var}\\left(\\epsilon\\right)\\\\\n",
    "&= \\left[\\text{Bias}\\left(\\hat{f}(x_0)\\right)\\right]^2 + \\text{Var}\\left(\\hat{f}(x_0)\\right)+\\text{Var}\\left(\\epsilon\\right)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4ea836",
   "metadata": {},
   "source": [
    "This equation tells us that in order to minimize the expected test error, we should aim for a statistical method that simultaneously achieves low variance and low bias. We note that variance is inherently a nonnegative quantity, and squared bias is also nonnegative. Hence, we see that the expected test MSE can never lie below $\\operatorname{Var}(\\epsilon)$, the irreducible error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9961f24c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efeb1f0e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
